# ðŸŽ¯ Backend Integration: Orchestration Workflow

**Status:** Ready to execute  
**Framework:** F-CONSOLE-0.1  
**Console Version:** 0.1.0 (PRODUCTION-READY)  
**Backend Phase:** CONTRACT ALIGNMENT  

---

## ðŸ“‹ WORKFLOW OVERVIEW

```
â”Œâ”€ PHASE 1: PROMPT & INTAKE (NOW) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  1. Send orchestration prompt to Claude      â”‚
â”‚     Sonnet (Backend Dev Senior)              â”‚
â”‚  2. Expect response with 3 sections:         â”‚
â”‚     â€¢ Backend inventory (factual)            â”‚
â”‚     â€¢ Endpoint gap matrix                    â”‚
â”‚     â€¢ Critical gaps list                     â”‚
â”‚  3. Receive response                         â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€ PHASE 2: EVALUATION (IMMEDIATE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  1. Use EVALUATION_TEMPLATE.md to grade      â”‚
â”‚  2. Compare against IDEAL_RESPONSE_REFERENCE â”‚
â”‚  3. Identify gaps, blockers, surprises       â”‚
â”‚  4. Make GO/NO-GO decision                   â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€ PHASE 3: SPEC GENERATION (IF GO) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚  1. Create docs/INTEGRATION_SPEC.md          â”‚
â”‚     â”œâ”€ Detailed adapter endpoints            â”‚
â”‚     â”œâ”€ Code examples (response format)       â”‚
â”‚     â””â”€ Testing scenarios                     â”‚
â”‚  2. Create docs/BACKEND_CONTRACT_GAPS.md     â”‚
â”‚     â”œâ”€ Gap list (prioritized)                â”‚
â”‚     â”œâ”€ Fix procedures (step-by-step)         â”‚
â”‚     â””â”€ Acceptance criteria                   â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€ PHASE 4: IMPLEMENTATION (BACKEND TEAM) â”€â”€â”€â”
â”‚                                             â”‚
â”‚  Backend Dev Senior implements per:         â”‚
â”‚  âœ“ Orchestration prompt                     â”‚
â”‚  âœ“ Integration spec                         â”‚
â”‚  âœ“ Gap fixes list                           â”‚
â”‚                                             â”‚
â”‚  Deliverables:                              â”‚
â”‚  âœ“ 4 public endpoints (/api/execute, etc.)  â”‚
â”‚  âœ“ Integration tests                        â”‚
â”‚  âœ“ Updated docs                             â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€ PHASE 5: VALIDATION & DEPLOYMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚  Console Dev Lead validates:                â”‚
â”‚  1. Test endpoints against contract         â”‚
â”‚  2. Run integration test suite               â”‚
â”‚  3. Verify error scenarios                  â”‚
â”‚  4. docker-compose full stack test          â”‚
â”‚                                             â”‚
â”‚  If PASS â†’ Go to deployment                 â”‚
â”‚  If FAIL â†’ Loop back to implementation      â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€ PHASE 6: PRODUCTION DEPLOYMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                             â”‚
â”‚  1. Both images built & tagged              â”‚
â”‚  2. docker-compose up tested locally        â”‚
â”‚  3. docker network verified (techno-net)    â”‚
â”‚  4. API endpoints verified                  â”‚
â”‚  5. Audit trail verified                    â”‚
â”‚  6. Deploy to production                    â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“š DOCUMENTS TO USE (IN ORDER)

### PHASE 1: Prompt & Intake

**File:** `docs/BACKEND_ORCHESTRATION_PROMPT.md`

**What:** Official orchestration prompt for Claude Sonnet  
**Why:** Structured intake, prevents scope creep, clear requirements  
**Action:** Copy & paste into your Backend Dev conversation  

**Key sections:**
- âš ï¸ IMPORTANT CONTEXT (contract alignment, not greenfield)
- ðŸ“‹ SECTION 1: Backend inventory template
- ðŸ“‹ SECTION 2: Endpoint gap matrix template
- ðŸ“‹ SECTION 3: Critical gaps list template

---

### PHASE 2: Evaluation

**File:** `docs/BACKEND_RESPONSE_EVALUATION_TEMPLATE.md`

**What:** Objective grading rubric for backend response  
**Why:** Fair, criterion-based evaluation (not subjective)  
**Action:** Fill out after receiving backend response

**Sections:**
- âœ… Section 1 assessment (each layer)
- âœ… Section 2 assessment (each endpoint)
- âœ… Section 3 assessment (gaps prioritized)
- ðŸ“Š Overall readiness score

**Output:** GO/NO-GO decision + timeline estimate

---

### PHASE 3: Reference Comparison

**File:** `docs/BACKEND_IDEAL_RESPONSE_REFERENCE.md`

**What:** "Gold standard" response for comparison  
**Why:** Know what GOOD looks like before you see REAL  
**Action:** Read BEFORE receiving actual response, then compare

**Includes:**
- ðŸŽ¯ Ideal Section 1 (inventory with specifics)
- ðŸŽ¯ Ideal Section 2 (gap matrix, low effort)
- ðŸŽ¯ Ideal Section 3 (0 critical gaps, 2-3 week timeline)
- ðŸš¨ Red flags (what would indicate problems)

---

## ðŸŽ¯ STEP-BY-STEP EXECUTION GUIDE

### STEP 1: Prepare & Send Prompt (5 min)

```bash
1. Open docs/BACKEND_ORCHESTRATION_PROMPT.md
2. Copy entire prompt
3. Start conversation with Claude Sonnet
4. Paste prompt
5. Add context: "Backend repo at d:\Projects\techno-os-backend"
6. Send message
```

**Expected response time:** 30-60 minutes for detailed analysis

---

### STEP 2: Receive & Document Response (10 min)

```bash
1. Receive response from Claude Sonnet
2. Copy response into new file:
   docs/BACKEND_ACTUAL_RESPONSE_[DATE].md
3. Save for comparison
4. Read entire response carefully
5. Highlight any surprises or concerns
```

---

### STEP 3: Evaluate Using Template (15 min)

```bash
1. Open docs/BACKEND_RESPONSE_EVALUATION_TEMPLATE.md
2. Read Sections 1, 2, 3 of ACTUAL response
3. Check each checkbox in evaluation template
4. Fill in [describe if not passing] sections
5. Complete "Overall Assessment" section
6. Save filled template as:
   docs/BACKEND_EVALUATION_RESULTS_[DATE].md
```

---

### STEP 4: Compare Against Ideal (10 min)

```bash
1. Open docs/BACKEND_IDEAL_RESPONSE_REFERENCE.md
2. Compare ACTUAL vs. IDEAL for each section
3. Document deviations:
   - Expected differences (ok to differ)
   - Red flags (escalate immediately)
   - Gaps vs. risks (medium concern)
4. Note any red flags in evaluation results
```

---

### STEP 5: Make GO/NO-GO Decision (5 min)

Based on evaluation + comparison, decide:

**ðŸŸ¢ GO** (Proceed to Phase 3)
- âœ… All 3 sections complete & detailed
- âœ… Critical gaps: 0 or very minor
- âœ… Timeline: â‰¤3 weeks for integration
- âœ… No redesign of existing governance layers
- âœ… No red flags

**ðŸŸ¡ PROCEED WITH CAUTION** (Need clarification)
- âš ï¸ Some details missing/unclear
- âš ï¸ Minor gaps (1-2) with clear fixes
- âš ï¸ Timeline: 2-4 weeks (acceptable)
- âš ï¸ Suggests minor adjustments (not redesign)
- âš ï¸ No red flags

**ðŸ”´ HOLD** (Need escalation)
- âŒ Critical gaps identified (3+)
- âŒ Red flags present
- âŒ Suggests redesign of existing layers
- âŒ Timeline: 4+ weeks (unacceptable)
- âŒ Incomplete response (not detailed)

---

### STEP 6: IF GO â†’ Create Integration Spec (1-2 hours)

```bash
1. Using backend response + evaluation results
2. Create docs/INTEGRATION_SPEC.md:
   â”œâ”€ Adapter endpoints (detailed)
   â”œâ”€ Response schemas (with examples)
   â”œâ”€ Testing scenarios (happy path + errors)
   â”œâ”€ Docker composition (both services)
   â””â”€ Deployment checklist

3. Create docs/BACKEND_CONTRACT_GAPS.md:
   â”œâ”€ Prioritized gap list
   â”œâ”€ Fix procedures (step-by-step)
   â”œâ”€ Code examples
   â””â”€ Acceptance criteria

4. Send both specs to Backend Dev Senior
5. Clarify any open questions
```

---

### STEP 7: Backend Team Implements (Timeline per response)

Backend Dev Senior:
- âœ“ Implements 4 public endpoints
- âœ“ Follows integration specs exactly
- âœ“ Writes integration tests
- âœ“ Updates documentation
- âœ“ Delivers code in docker-compose format

---

### STEP 8: Integration Testing (1-2 days)

Console Dev Lead:
```bash
1. Pull backend code
2. Update docker-compose to include both services
3. docker-compose up --build
4. Run integration tests:
   â”œâ”€ POST /api/execute (success path)
   â”œâ”€ GET /api/audit (success path)
   â”œâ”€ GET /api/memory (success path)
   â”œâ”€ Error scenarios (4XX, 5XX, timeout)
   â”œâ”€ Fail-closed behavior (unknown status)
   â””â”€ Audit trail logging
5. If all PASS â†’ deployment ready
6. If FAIL â†’ loop back to backend team
```

---

## ðŸ“Š DECISION MATRIX

| Evaluation Result | Action | Timeline |
|------------------|--------|----------|
| ðŸŸ¢ GO | Create specs, backend implements | 2-3 weeks |
| ðŸŸ¡ CAUTION | Request clarification, minor specs | 3-4 weeks |
| ðŸ”´ HOLD | Escalate, discuss trade-offs | 4+ weeks |

---

## ðŸŽ¯ SUCCESS CRITERIA (End of Phase 3)

When specs are created, backend work is:

âœ… Clearly defined (no ambiguity)  
âœ… Scoped (4 endpoints, adapter pattern)  
âœ… Testable (test scenarios in spec)  
âœ… Documented (examples + expected responses)  
âœ… Aligned with F-CONSOLE-0.1 (no governance changes)  
âœ… Timeline-bounded (2-3 weeks max)  

---

## ðŸ“ FILES CREATED FOR THIS WORKFLOW

```
d:\Projects\techno-os-console\docs\
â”œâ”€ BACKEND_ORCHESTRATION_PROMPT.md
â”‚  â””â”€ The prompt to send to Claude Sonnet
â”‚
â”œâ”€ BACKEND_RESPONSE_EVALUATION_TEMPLATE.md
â”‚  â””â”€ Grading rubric (fill after response)
â”‚
â”œâ”€ BACKEND_IDEAL_RESPONSE_REFERENCE.md
â”‚  â””â”€ Gold standard (read before response)
â”‚
â”œâ”€ BACKEND_INTEGRATION_WORKFLOW.md (this file)
â”‚  â””â”€ Step-by-step execution guide
â”‚
â”œâ”€ [Generated during workflow]
â”œâ”€ BACKEND_ACTUAL_RESPONSE_[DATE].md
â”œâ”€ BACKEND_EVALUATION_RESULTS_[DATE].md
â”œâ”€ INTEGRATION_SPEC.md (Phase 3)
â””â”€ BACKEND_CONTRACT_GAPS.md (Phase 3)
```

---

## ðŸŽ“ KEY PRINCIPLES FOR THIS WORKFLOW

1. **Structured intake:** Prompt prevents scope creep
2. **Objective evaluation:** Template prevents bias
3. **Reference baseline:** Ideal response sets expectations
4. **Clear decision gate:** GO/NO-GO decision is explicit
5. **Documented specs:** Backend has zero ambiguity
6. **Testable output:** Integration tests verify alignment

---

## ðŸš€ READY TO EXECUTE?

### Checklist before sending prompt:

- [ ] Have you read BACKEND_ORCHESTRATION_PROMPT.md?
- [ ] Have you read BACKEND_IDEAL_RESPONSE_REFERENCE.md?
- [ ] Do you have backend workspace ready (d:\Projects\techno-os-backend)?
- [ ] Do you understand the 3-section response format?
- [ ] Do you have evaluation template prepared?

### If all checked â†’ Ready to proceed!

```
Next action: Send BACKEND_ORCHESTRATION_PROMPT.md to Claude Sonnet
Expected: Detailed response with inventory + gaps + timeline
Evaluation: Use RESPONSE_EVALUATION_TEMPLATE.md
Decision: GO/HOLD based on evaluation results
```

---

**Framework:** F-CONSOLE-0.1 (Governance + Quality Gates)  
**Status:** Ready for backend integration phase  
**Risk Level:** Low (contract-driven, not greenfield)  
**Escalation:** If red flags â†’ escalate to human decision-makers

> **"IA como instrumento. Humano como centro."**
>
> This workflow ensures AI agents (Console Dev + Backend Dev) stay aligned
> through explicit contracts, objective evaluation, and human oversight.
